{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sC-LZ20S_WUr"
   },
   "source": [
    "# Information Extraction - Assignment\n",
    "This assignment is based on the Information Extraction lecture and the lab.\n",
    "\n",
    "Name : Sai Krishna Lakshminarayanan\n",
    "\n",
    "Student ID: 18230229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xqCFJBv_WUt"
   },
   "outputs": [],
   "source": [
    "#Replicating the same code as providied in the assignment question\n",
    "import nltk\n",
    "import re\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0mo13vi_WUx"
   },
   "outputs": [],
   "source": [
    "#Replicating the same code as providied in the assignment question\n",
    "inputfile='football_players.txt' #Location of the file\n",
    "buf=open(inputfile, encoding=\"UTF-8\")\n",
    "list_of_doc=buf.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCEJrJ-p_WU1"
   },
   "source": [
    "# Task 1 (10 Marks)\n",
    "Write a function that takes each document and performs:\n",
    "1) sentence segmentation 2) tokenization 3) part-of-speech tagging\n",
    "\n",
    "Please keep in mind that the expected output is a list within a list as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3MCJIcR_WU2"
   },
   "outputs": [],
   "source": [
    "line = []\n",
    "for i in list_of_doc:\n",
    "    if len(i) != 0:#removing the empty lines present in the given document\n",
    "        line.append(i)\n",
    "list_of_doc = line\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def ie_preprocess(document):#to preprocess the document\n",
    "    #code goes here\n",
    "    sentence = sent_tokenize(document)# Converting string into list of sentences present in the document\n",
    "    pos_sentences = []\n",
    "    for i in sentence:\n",
    "        text = word_tokenize(i) # Tokenizing every sentence in the document\n",
    "        pos_sentences.append(nltk.pos_tag(text)) # Tagging of the tokenizeed sentence\n",
    "    return pos_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-E04CUNb_WU6"
   },
   "source": [
    "Run the following code to check your result for the first document (Ronaldo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R30taRgf_WU6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('forward', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('serves', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('captain', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Portugal', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the above code\n",
    "first_doc=list_of_doc[0]\n",
    "pos_sent=ie_preprocess(first_doc)\n",
    "pos_sent[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMw70Wm8_WU-"
   },
   "source": [
    "Expected output\n",
    " [...[('He', 'PRP'),\n",
    "  ('is', 'VBZ'),\n",
    "  ('a', 'DT'),\n",
    "  ('forward', 'NN'),\n",
    "  ('and', 'CC'),\n",
    "  ('serves', 'NNS'),\n",
    "  ('as', 'IN'),\n",
    "  ('captain', 'NN'),\n",
    "  ('for', 'IN'),\n",
    "  ('Portugal', 'NNP'),\n",
    "  ('.', '.')], ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYTwrZId_WU_"
   },
   "source": [
    "# Task 2 (20 Marks)\n",
    "Write a function that will take the list of tokens with POS tags for each sentence and returns the named entities (NE). \n",
    "\n",
    "Hint: Use binary=True while calling NE chunk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fC0iqJJ_WU_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cristiano Ronaldo',\n",
       " 'Santos Aveiro',\n",
       " 'ComM',\n",
       " 'GOIH',\n",
       " 'Portuguese',\n",
       " 'Spanish',\n",
       " 'Real Madrid',\n",
       " 'Portugal']"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function will be called later in task 3, 4 also for searching Organisation and others\n",
    "#Therefore two extra parameters x and y are adding inorder to ensure this function can be used there too\n",
    "#By default x is true and y is \"NE\" in order to fulfill the task 2 directly without specifying x and y\n",
    "def named_entity_finding(pos_sent, x=True, y=\"NE\"):#to find the named entities in the document\n",
    "    named_entities=[]\n",
    "    b = nltk.ne_chunk(pos_sent, binary=x)#In this binary is equal to the value given by user. By default, binary=True\n",
    "    for subtree in b.subtrees():\n",
    "        if subtree.label() == y:#when the value is equal to the given condition. By default, y = \"NE\"\n",
    "            a = \"\"\n",
    "            for leaf in subtree.leaves():\n",
    "                a = a + leaf[0] + \" \"\n",
    "            named_entities.append(a.strip())#Adding all the named entities\n",
    "    return named_entities\n",
    "#Cross checking the above code\n",
    "pos_sents=ie_preprocess(list_of_doc[0])\n",
    "named_entity_finding(pos_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDolbFKE_WVD"
   },
   "source": [
    "Expected output ['Cristiano Ronaldo',\n",
    " 'Santos Aveiro',\n",
    " 'ComM',\n",
    " 'GOIH',\n",
    " 'Portuguese',\n",
    " 'Portuguese',\n",
    " 'Spanish',\n",
    " 'Real Madrid',\n",
    " 'Portugal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHMp7xtK_WVE"
   },
   "source": [
    "# Task 3 (10 Marks)\n",
    "\n",
    "Now use the named_entity_finding() function to extract all NEs for each document.\n",
    "\n",
    "Hint: pos_sents holds the list of lists of tokens with POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwFlzQx4_WVF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Luis Su√°rez', 'PSG', 'Germany']"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "def NE_flat_list_fn(pos_sents): \n",
    "    NE=[]\n",
    "    for i in pos_sents:\n",
    "            pos_sents = ie_preprocess(i)#tagging every sentence in the document using the for loop\n",
    "            for pos_sent in pos_sents:\n",
    "                a = named_entity_finding(pos_sent)#Finding named entities for each sentence in the document\n",
    "                if len(a) != 0:\n",
    "                    NE.append(a)#storing it in NE[] by appending the values of a\n",
    "    #Single line code here. Flatten the list of lists to the single list NE_flat_list\n",
    "    NE_flat_list = list(itertools.chain.from_iterable(NE))\n",
    "    return NE_flat_list\n",
    "ne_flat = NE_flat_list_fn(list_of_doc)\n",
    "list(set(ne_flat))[4:7]# Taking the unqiue named entities by using set function and showing them as a list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7-ma9lJ_WVJ"
   },
   "source": [
    "# Task 4 (40 Marks)\n",
    "\n",
    "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
    "\n",
    "Hint: Use the re.compile() function to create the extraction patterns\n",
    "\n",
    "Reference: https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David Robert Joseph Beckham'"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def name_of_the_player(doc):#to find the name of the player\n",
    "    t = []\n",
    "    name = []\n",
    "\n",
    "    pos_sents = ie_preprocess(doc)\n",
    "    tree = nltk.ne_chunk(pos_sents[0])\n",
    "\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == \"PERSON\":#when the value is equal to the PERSON entity\n",
    "            a = \"\"#storing the names in a\n",
    "            for leaf in subtree.leaves():\n",
    "                a = a + leaf[0] + \" \"\n",
    "            name.append(a.strip())# appending the values of a into name\n",
    "    name=\" \".join(name)\n",
    "    return name # joining the segment of the name into complete one\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name_of_the_player(list_of_doc[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'England'"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def country_of_origin(doc):#to obtain the country of origin of the player\n",
    "    sent = sent_tokenize(doc)[0]\n",
    "    a = []\n",
    "    e = []\n",
    "      \n",
    "    b = re.search(r'((?:\\w+\\W+){,3})(footballer)', sent)#searcing using regex to find the nationality by preceding word of footballer\n",
    "    s = \"\".join(list(b.groups()))\n",
    "    c = re.search(r'((\\w+ ){1})professional', s)#searcing using regex to find the nationality by preceding word of professional\n",
    "    c = c.groups()[0]\n",
    "    tag = ie_preprocess(s)[0]#tagging of the sentences\n",
    "    tree = nltk.ne_chunk(tag)\n",
    "    born = []\n",
    "\n",
    "    for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'GPE':#checking for the GPE tagging entities in the sentences\n",
    "                entity = \"\"\n",
    "                for leaf in subtree.leaves():\n",
    "                    entity = entity + leaf[0] + \" \"\n",
    "                born.append(entity.strip())#appending the values to born\n",
    "\n",
    "    if len(born) == 0:\n",
    "        if len(c) != 0:#when nltk can't find the country before, now using ways of searching it by finding it near player in the document\n",
    "            born = nltk.word_tokenize(c) \n",
    "    d = ['Spain', 'England', 'Portugal', 'Wales', 'England', 'Brazil', 'Germany', 'Argentina', 'Sweden']#countries present in document\n",
    "    for x in d:\n",
    "        e.append(SequenceMatcher(None, born[0], x).ratio())\n",
    "    country = d[e.index(max(e))]#obtaining the country value\n",
    "\n",
    "\n",
    "    return country\n",
    "\n",
    "country_of_origin(list_of_doc[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_of_birth(doc):#To obtain the date of birth of the player\n",
    "    #code goes here\n",
    "    sentence = sent_tokenize(doc)[0]#tokenization of the sentence\n",
    "    match = re.compile(r'born\\b\\s*((?:\\S+\\s+){0,3})')#using regex to extract the values\n",
    "    date = match.findall(sentence)[0]\n",
    "    date = re.sub('\\W+',' ', date )\n",
    "    \n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-CNrMM5_WVO"
   },
   "source": [
    "Execute the below command to check your fuction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpeKE1u9_WVP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 February 1992 '"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_of_birth(list_of_doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output '5 February 1992'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['England national team']"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def team_of_the_player(doc):#to obtain the team of the player\n",
    "    \n",
    "    sent = sent_tokenize(doc)\n",
    "    tm = []\n",
    "    a = []\n",
    "    b = []\n",
    "\n",
    "    tag = ie_preprocess(doc)\n",
    "    for i, sent in enumerate(sent):#checking the initial lines\n",
    "        if i == 2:\n",
    "            break\n",
    "        m = re.compile(r'((?:[\\S,]+\\s+){0,1})national team')#regex to get national team\n",
    "        m_a = re.compile(r'a\\s+((?:[\\S,]+\\s*){0,2})')#regex to get club\n",
    "\n",
    "        team = m.findall(sent)#finding national team\n",
    "\n",
    "        if len(m_a.findall(sent)) != 0:\n",
    "            for f in m_a.findall(sent):\n",
    "                b.append(f)#appending clubs based on key words\n",
    "\n",
    "        a.append(named_entity_finding(tag[i], False, \"ORGANIZATION\"))\n",
    "        if len(team) != 0:\n",
    "            l = team[0]+\"national team\"\n",
    "            tm.append(l)#finding national team\n",
    "\n",
    "    if len(list(set(tm))) != 0:\n",
    "        n = list(set(tm))[0]#obtaining the unique list of values\n",
    "    else:\n",
    "        n = country_of_origin(doc) +\" national team\"\n",
    "\n",
    "    a = list(itertools.chain.from_iterable(a))   # Flattening the list\n",
    "\n",
    "    nclub = []\n",
    "    for q in b:\n",
    "        nclub.extend(nltk.word_tokenize(q))\n",
    "\n",
    "    for i, s in enumerate(b):\n",
    "        b[i] = b[i].rstrip()\n",
    "    if len(list(set(a).intersection(b))) != 0:\n",
    "        a = list(set(a).intersection(b))\n",
    "    a.append(n)#appending the values\n",
    "\n",
    "\n",
    "    team = list(set(a).difference(nltk.word_tokenize(sent[0])[0:6]))#obtaining the team\n",
    "\n",
    "    return team\n",
    "\n",
    "\n",
    "team_of_the_player(list_of_doc[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forward']"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def position_of_the_player(doc):#to obtain the position of the player\n",
    "    pos = [\"forward\", \"captian\", \"attacking midfielder\", \"striker\", \"winger\", \"central midfielder\", \"defensive tackle\", \"defensive end\"]\n",
    "    position = []#various positions in football is generated as a list\n",
    "    sent = sent_tokenize(doc)#tokenization of the sentence\n",
    "    for i, sent in enumerate(sent):\n",
    "        for x in pos:\n",
    "            rex = re.compile(r'\\b({0})\\b'.format(x), flags=re.IGNORECASE)#using regex to find matching values\n",
    "            r = bool(rex.search(sent))\n",
    "            if r == True:\n",
    "                position.append(x)#appending to position when value is true\n",
    "\n",
    "    return position\n",
    "\n",
    "position_of_the_player(list_of_doc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCYflDpp_WVU"
   },
   "source": [
    "# Task 5 (10 Marks)\n",
    "\n",
    "Write a function using the outputs from the previous functions to generate JSON-LD output as follows.\n",
    "\n",
    "Reference: https://json-ld.org/primer/latest/\n",
    "\n",
    "{ \"@id\": \"http://my-soccer-ontology.com/footballer/name_of_the_player\",\n",
    "\n",
    "    \"name\": \"\",\n",
    "    \"born\": \"\",\n",
    "    \"country\": \"\",\n",
    "    \"position\": [\n",
    "        { \"@id\": \"http://my-soccer-ontology.com/position\",\n",
    "            \"type\": \"\"\n",
    "        }\n",
    "     ]   \n",
    "     \"team\": [\n",
    "        { \"@id\": \"http://my-soccer-ontology.com/team\",\n",
    "            \"name\": \"\"\n",
    "        }   \n",
    "     ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8HStdlu_WVU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wayne Mark Rooney', '24 October 1985 ', 'England', ['forward'], ['England national team']]\n"
     ]
    }
   ],
   "source": [
    "from pyld import jsonld\n",
    "import json\n",
    "def data_generator(doc):#generating the data\n",
    "    \n",
    "    data = [name_of_the_player(doc), date_of_birth(doc), country_of_origin(doc), position_of_the_player(doc), team_of_the_player(doc)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "data = data_generator(list_of_doc[4])#storing it here\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"@id\": \"http://my-soccer-ontology.com/footballer/ Wayne Mark Rooney\",\n",
      "   \"name\": \"Wayne Mark Rooney\",\n",
      "   \"born\": \"24 October 1985 \",\n",
      "   \"country\": \"England\",\n",
      "   \"position\": [\n",
      "      {\n",
      "         \"@id\": \"http://my-soccer-ontology.com/position/\",\n",
      "         \"type\": [\n",
      "            \"forward\"\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"team\": [\n",
      "      {\n",
      "         \"@id\": \"http://my-soccer-ontology.com/team/\",\n",
      "         \"name\": [\n",
      "            \"England national team\"\n",
      "         ]\n",
      "      }\n",
      "   ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def generate_jsonld(arg):#displaying the data based on json ld\n",
    "    ld = { \"@id\": \"http://my-soccer-ontology.com/footballer/ \"+arg[0],\n",
    "\n",
    "        \"name\": arg[0],\n",
    "        \"born\": arg[1],\n",
    "        \"country\": arg[2],\n",
    "        \"position\": [\n",
    "            { \"@id\": \"http://my-soccer-ontology.com/position/\",\n",
    "                \"type\": arg[3]\n",
    "            }\n",
    "         ],   \n",
    "         \"team\": [\n",
    "            { \"@id\": \"http://my-soccer-ontology.com/team/\",\n",
    "                \"name\": arg[4]\n",
    "            }   \n",
    "         ],\n",
    "           \n",
    "    }\n",
    "\n",
    "    return json.dumps(ld,indent = 3, separators=(',', ': '))\n",
    "\n",
    "\n",
    "data = data_generator(list_of_doc[4])\n",
    "print(generate_jsonld(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3VtWxBr_WVZ"
   },
   "source": [
    "# Task 6 (10 Marks)\n",
    "Identify one other relation (besides team and player) and write a function to extract this. Also extend the JSON-LD output accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2002']"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def debutyear(doc):#to obtain the debut year of the player\n",
    "    sent = sent_tokenize(doc)\n",
    "    deb = []\n",
    "    ag = []\n",
    "    for se in sent:\n",
    "        sp_sent = se.split()\n",
    "        if \"debut\" in sp_sent:\n",
    "            date = re.findall('\\d{4}', \" \".join(sp_sent))#when debut year is present finding it using regex\n",
    "            if len(date) !=0:\n",
    "                deb.append(date[0])#appending the value of the year\n",
    "    if len(deb) == 0:#when debut year is not given\n",
    "        deb.append(\"Not Available\")\n",
    "   \n",
    "    return [deb[0]]\n",
    "debutyear(list_of_doc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wayne Mark Rooney', '24 October 1985 ', 'England', ['forward'], ['England national team'], ['2002']]\n"
     ]
    }
   ],
   "source": [
    "def data_generator(doc):#generating the data along with debut year\n",
    "    data = [name_of_the_player(doc), date_of_birth(doc), country_of_origin(doc), position_of_the_player(doc), team_of_the_player(doc), debutyear(doc)]\n",
    "    return data\n",
    "data = data_generator(list_of_doc[4])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"@id\": \"http://my-soccer-ontology.com/footballer/ Wayne Mark Rooney\",\n",
      "   \"name\": \"Wayne Mark Rooney\",\n",
      "   \"born\": \"24 October 1985 \",\n",
      "   \"country\": \"England\",\n",
      "   \"position\": [\n",
      "      {\n",
      "         \"@id\": \"http://my-soccer-ontology.com/position/\",\n",
      "         \"type\": [\n",
      "            \"forward\"\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"team\": [\n",
      "      {\n",
      "         \"@id\": \"http://my-soccer-ontology.com/team/\",\n",
      "         \"name\": [\n",
      "            \"England national team\"\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"Debut Year\": [\n",
      "      \"2002\"\n",
      "   ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def generate_jsonld(arg):#adding debut year to the json ld\n",
    "\n",
    "\n",
    "        ld = { \"@id\": \"http://my-soccer-ontology.com/footballer/ \"+arg[0],\n",
    "\n",
    "            \"name\": arg[0],\n",
    "            \"born\": arg[1],\n",
    "            \"country\": arg[2],\n",
    "            \"position\": [\n",
    "                { \"@id\": \"http://my-soccer-ontology.com/position/\",\n",
    "                    \"type\": arg[3]\n",
    "                }\n",
    "             ],   \n",
    "             \"team\": [\n",
    "                { \"@id\": \"http://my-soccer-ontology.com/team/\",\n",
    "                    \"name\": arg[4]\n",
    "                }   \n",
    "             ],\n",
    "               \"Debut Year\": arg[5]\n",
    "        }\n",
    "\n",
    "        return json.dumps(ld,indent = 3, separators=(',', ': '))\n",
    "    \n",
    "\n",
    "data = data_generator(list_of_doc[4])\n",
    "print(generate_jsonld(data))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
